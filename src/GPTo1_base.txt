# Bayesian Estimation of Mixture Ratios in Mitochondrial DNA Samples

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import binom
from scipy.interpolate import interp1d

# **1. Simulation of Mitochondrial Genomes for Samples R and D**

# Parameters
L = 16569          # Length of mitochondrial genome (human mtDNA)
M = 100            # Number of informative positions
mean_depth = 200   # Mean sequencing depth
epsilon = 0.01     # Sequencing error rate
alleles = ['A', 'C', 'G', 'T']
np.random.seed(42) # For reproducibility

# Generate informative positions (positions where D differs from R)
positions = np.arange(1, M+1)

# Assign alleles to sample R
alleles_R = np.random.choice(alleles, M)

# Assign different alleles to sample D at the same positions
alleles_D = []
for allele_R in alleles_R:
    other_alleles = [a for a in alleles if a != allele_R]
    allele_D = np.random.choice(other_alleles)
    alleles_D.append(allele_D)
alleles_D = np.array(alleles_D)

# **2. Function to Simulate Sequencing Reads for Mixed Sample RDM**

def simulate_reads(p_true, M, mean_depth, epsilon):
    """
    Simulate sequencing reads for the mixed sample RDM.
    """
    # Simulate total reads at each position
    n_reads = np.random.poisson(lam=mean_depth, size=M)
    
    # Expected frequency of D-specific allele at each position
    f_i = p_true * (1 - epsilon) + (1 - p_true) * epsilon
    
    # Simulate number of reads supporting D-specific allele at each position
    k_reads = np.random.binomial(n_reads, f_i)
    
    return k_reads, n_reads

# **3. Function to Compute Log Likelihood**

def log_likelihood(p, k_reads, n_reads, epsilon):
    """
    Compute the log likelihood of observing the data given mixture proportion p.
    """
    f_i = p * (1 - epsilon) + (1 - p) * epsilon
    f_i = np.clip(f_i, 1e-10, 1 - 1e-10)  # Avoid log(0)
    log_L_i = binom.logpmf(k_reads, n_reads, f_i)
    log_L = np.sum(log_L_i)
    return log_L

# **4. Function to Perform Bayesian Inference**

def bayesian_estimation(k_reads, n_reads, epsilon):
    """
    Perform Bayesian estimation of the mixture proportion p.
    """
    # Define a grid of p values
    p_grid = np.linspace(0, 0.05, 500)
    
    # Compute log likelihood for each p
    log_L_values = np.array([log_likelihood(p, k_reads, n_reads, epsilon) for p in p_grid])
    
    # Convert log likelihoods to likelihoods
    L_values = np.exp(log_L_values - np.max(log_L_values))  # For numerical stability
    
    # Assume a uniform prior over p (from 0% to 5%)
    posterior = L_values / np.sum(L_values)
    
    # Compute estimates
    p_est_mean = np.sum(p_grid * posterior)
    cumulative_posterior = np.cumsum(posterior)
    interp_cdf = interp1d(cumulative_posterior, p_grid)
    p_est_median = interp_cdf(0.5)
    p_est_mode = p_grid[np.argmax(posterior)]
    lower_bound = interp_cdf(0.025)
    upper_bound = interp_cdf(0.975)
    
    return {
        'p_grid': p_grid,
        'posterior': posterior,
        'p_est_mean': p_est_mean,
        'p_est_median': p_est_median,
        'p_est_mode': p_est_mode,
        'lower_bound': lower_bound,
        'upper_bound': upper_bound
    }

# **5. Simulation and Estimation Function**

def simulate_and_estimate(p_true, M, mean_depth, epsilon):
    """
    Simulate data and estimate the mixture proportion p.
    """
    # Simulate sequencing reads
    k_reads, n_reads = simulate_reads(p_true, M, mean_depth, epsilon)
    
    # Perform Bayesian estimation
    estimation_results = bayesian_estimation(k_reads, n_reads, epsilon)
    
    # Include true p in results
    estimation_results['p_true'] = p_true
    return estimation_results

# **6. Validation with Simulated Data**

# Test mixture proportions
p_true_values = [0.001, 0.005, 0.01, 0.02]

# Collect results
results_list = []

for p_true in p_true_values:
    results = simulate_and_estimate(p_true, M, mean_depth, epsilon)
    results_list.append(results)
    print(f"True p: {p_true*100:.2f}%")
    print(f"Estimated p (mean): {results['p_est_mean']*100:.4f}%")
    print(f"Estimated p (median): {results['p_est_median']*100:.4f}%")
    print(f"Estimated p (mode): {results['p_est_mode']*100:.4f}%")
    print(f"95% Credible Interval: [{results['lower_bound']*100:.4f}%, {results['upper_bound']*100:.4f}%]")
    print("-" * 50)

# **7. Plotting Posterior Distributions**

plt.figure(figsize=(12, 8))
for results in results_list:
    plt.plot(results['p_grid']*100, results['posterior'], label=f"True p = {results['p_true']*100:.2f}%")
plt.xlabel('Mixture Proportion p (%)')
plt.ylabel('Posterior Probability')
plt.title('Posterior Distributions for Different True Mixture Proportions')
plt.legend()
plt.show()

# **8. Performance Evaluation**

# Extract true and estimated p values
true_p_values = [res['p_true'] for res in results_list]
estimated_p_means = [res['p_est_mean'] for res in results_list]
estimated_p_medians = [res['p_est_median'] for res in results_list]
estimated_p_modes = [res['p_est_mode'] for res in results_list]

# Plot True vs. Estimated p
plt.figure(figsize=(10, 6))
plt.plot(np.array(true_p_values)*100, np.array(estimated_p_means)*100, 'o-', label='Estimated p (Mean)')
plt.plot(np.array(true_p_values)*100, np.array(estimated_p_medians)*100, 's--', label='Estimated p (Median)')
plt.plot(np.array(true_p_values)*100, np.array(estimated_p_modes)*100, 'd:', label='Estimated p (Mode)')
plt.plot([0, 2], [0, 2], 'k--', label='Ideal')
plt.xlabel('True Mixture Proportion p (%)')
plt.ylabel('Estimated Mixture Proportion p (%)')
plt.title('True vs. Estimated Mixture Proportions')
plt.legend()
plt.show()

# **9. Sensitivity Analysis**

# Vary sequencing depth
depth_values = [50, 100, 200, 500]
p_true = 0.005  # Fixed true p at 0.5%
results_depth = []

for depth in depth_values:
    results = simulate_and_estimate(p_true, M, depth, epsilon)
    results_depth.append(results)
    print(f"Sequencing Depth: {depth}x")
    print(f"Estimated p (mean): {results['p_est_mean']*100:.4f}%")
    print(f"95% Credible Interval: [{results['lower_bound']*100:.4f}%, {results['upper_bound']*100:.4f}%]")
    print("-" * 50)

# **10. Conclusion**

# The Bayesian method accurately estimates mixture proportions as low as 0.1% with sufficient read depth and number of informative positions. The method is robust to sequencing errors due to incorporation of the error rate in the model.

